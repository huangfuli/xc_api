{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e137b7-79cc-40cd-a542-ac8c9ad9a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 1. 环境设置\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34a34a-db77-48bd-9553-df6e899125dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794cf9bb-ed51-421f-abf2-db84e6cfd775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 数据预处理（适配你的数据格式）\n",
    "# 读取Excel文件\n",
    "raw_data = pd.read_excel('data/002230.SZ_1min_data.xlsx')\n",
    "\n",
    "# 数据清洗\n",
    "def clean_data(df):\n",
    "    # 转换时间格式\n",
    "    df['trade_time'] = pd.to_datetime(df['trade_time'])\n",
    "    # 去重\n",
    "    df = df.drop_duplicates(subset=['trade_time'])\n",
    "    # 过滤异常值\n",
    "    df = df[(df['close'] > 0) & (df['vol'] > 0)]\n",
    "    # 设置时间索引\n",
    "    df = df.set_index('trade_time').sort_index()\n",
    "    # 重命名列\n",
    "    df = df.rename(columns={'vol': 'volume', 'amount': 'amount'})\n",
    "    return df\n",
    "\n",
    "cleaned_data = clean_data(raw_data)\n",
    "\n",
    "# 技术指标计算（增强版）\n",
    "def add_technical_indicators(df):\n",
    "    # 移动平均\n",
    "    df['MA5'] = df['close'].rolling(window=5).mean()\n",
    "    df['MA10'] = df['close'].rolling(window=10).mean()\n",
    "    df['MA20'] = df['close'].rolling(window=20).mean()\n",
    "    \n",
    "    # 波动率\n",
    "    df['Volatility'] = df['close'].pct_change().rolling(20).std() * np.sqrt(240)\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    exp12 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp26 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp12 - exp26\n",
    "    df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # 成交量变化率\n",
    "    df['Volume_Change'] = df['volume'].pct_change()\n",
    "    \n",
    "    # 价格动量\n",
    "    df['Momentum'] = df['close'] / df['close'].shift(4) - 1\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "feature_data = add_technical_indicators(cleaned_data)\n",
    "\n",
    "# 特征选择\n",
    "selected_features = [\n",
    "    'open', 'high', 'low', 'close', 'volume',\n",
    "    'MA5', 'MA10', 'MA20', 'Volatility',\n",
    "    'RSI', 'MACD', 'Signal', 'Volume_Change', 'Momentum'\n",
    "]\n",
    "\n",
    "# 数据标准化\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(feature_data[selected_features])\n",
    "\n",
    "# 转换为numpy数组\n",
    "processed_data = scaled_data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34f33ab-e2ca-4d92-87e3-f1639d7394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 修正强化学习环境类（替换之前的定义）\n",
    "class EnhancedStockTradingEnv:\n",
    "    def __init__(self, data, initial_balance=100000, transaction_cost=0.0005):\n",
    "        self.data = data\n",
    "        self.index = 0\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost  # 加入交易成本\n",
    "        \n",
    "        # 状态信息\n",
    "        self.balance = initial_balance  # 修正拼写错误\n",
    "        self.shares_held = 0\n",
    "        self.position_value = 0\n",
    "        self.total_value = [initial_balance]\n",
    "        self.trades = []\n",
    "        \n",
    "        # 状态维度：市场数据 + 持仓信息（3个） + 时间信息（1个）\n",
    "        self.state_size = data.shape[1] + 4\n",
    "    \n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.position_value = 0\n",
    "        self.total_value = [self.initial_balance]\n",
    "        self.trades = []\n",
    "        return self._next_state()\n",
    "    \n",
    "    def _next_state(self):\n",
    "        # 市场数据\n",
    "        market_state = self.data[self.index]\n",
    "        \n",
    "        # 持仓信息\n",
    "        position_state = [\n",
    "            self.shares_held / 1e4,  # 标准化持仓量\n",
    "            self.balance / self.initial_balance,\n",
    "            (self.total_value[-1] / self.initial_balance) - 1  # 收益率\n",
    "        ]\n",
    "        \n",
    "        # 时间信息（标准化到0-1）\n",
    "        time_state = [self.index / len(self.data)]\n",
    "        \n",
    "        return np.concatenate([market_state, position_state, time_state])\n",
    "    \n",
    "    def step(self, action):\n",
    "        current_price = self.data[self.index][3]  # close价格的位置\n",
    "        done = self.index == len(self.data)-1\n",
    "        reward = 0\n",
    "        trade_cost = 0\n",
    "        \n",
    "        # 动作执行\n",
    "        if action == 1:  # 买入\n",
    "            max_buy = self.balance // (current_price * (1 + self.transaction_cost))\n",
    "            if max_buy > 0:\n",
    "                self.shares_held += max_buy\n",
    "                trade_cost = max_buy * current_price * self.transaction_cost\n",
    "                self.balance -= max_buy * current_price * (1 + self.transaction_cost)\n",
    "                self.trades.append(('buy', self.index, current_price, max_buy))\n",
    "                \n",
    "        elif action == 2:  # 卖出\n",
    "            if self.shares_held > 0:\n",
    "                trade_cost = self.shares_held * current_price * self.transaction_cost\n",
    "                self.balance += self.shares_held * current_price * (1 - self.transaction_cost)\n",
    "                self.trades.append(('sell', self.index, current_price, self.shares_held))\n",
    "                self.shares_held = 0\n",
    "                \n",
    "        # 更新状态\n",
    "        self.index += 1\n",
    "        self.position_value = self.shares_held * current_price\n",
    "        new_total = self.balance + self.position_value\n",
    "        self.total_value.append(new_total)\n",
    "        \n",
    "        # 计算奖励（考虑夏普比率）\n",
    "        price_change = current_price / self.data[self.index-1][3] - 1 if self.index > 0 else 0\n",
    "        portfolio_return = (new_total / self.total_value[-2]) - 1\n",
    "        reward = portfolio_return - 0.5 * (price_change ** 2)  # 风险调整后收益\n",
    "        \n",
    "        # 添加终止条件\n",
    "        if new_total < self.initial_balance * 0.8:  # 最大回撤20%终止\n",
    "            done = True\n",
    "            reward -= 10\n",
    "            \n",
    "        return self._next_state(), reward, done, {'current_price': current_price}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd11fe44-3a55-4987-ae6d-39b9cccdef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 改进的DQN网络结构\n",
    "class EnhancedDQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(EnhancedDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        return value + advantage - advantage.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22af81b2-5120-4d33-b77d-3580f99584b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 补充经验回放缓存定义（在步骤5之前运行）\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for idx in indices:\n",
    "            s, a, r, ns, d = self.buffer[idx]\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            next_states.append(ns)\n",
    "            dones.append(d)\n",
    "        return (np.array(states, dtype=np.float32),\n",
    "                np.array(actions, dtype=np.int64),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.array(next_states, dtype=np.float32),\n",
    "                np.array(dones, dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 现在可以继续执行之前的步骤5代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d00eb93-b7de-4591-9864-bfda55e9336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 训练参数配置\n",
    "state_size = processed_data.shape[1] + 4  # 原始特征数 + 新增状态特征\n",
    "action_size = 3  # 0-持有，1-买入，2-卖出\n",
    "\n",
    "# 网络参数\n",
    "policy_net = EnhancedDQN(state_size, action_size).to(device)\n",
    "target_net = EnhancedDQN(state_size, action_size).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# 优化器\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=10)\n",
    "\n",
    "# 经验回放\n",
    "replay_buffer = ReplayBuffer(50000)\n",
    "\n",
    "# 训练参数\n",
    "batch_size = 512\n",
    "gamma = 0.97\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.997\n",
    "target_update = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391370b-bd8e-4a2d-8873-674ab686aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 改进的训练循环\n",
    "env = EnhancedStockTradingEnv(processed_data, transaction_cost=0.0005)\n",
    "episodes = 800\n",
    "best_score = -np.inf\n",
    "epsilon = epsilon_start\n",
    "\n",
    "# 训练记录\n",
    "training_log = {\n",
    "    'episode': [],\n",
    "    'total_return': [],\n",
    "    'max_drawdown': [],\n",
    "    'sharpe_ratio': [],\n",
    "    'epsilon': []\n",
    "}\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    episode_values = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 动态epsilon贪婪策略\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice([0, 1, 2], p=[0.4, 0.3, 0.3])  # 偏向持有\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        episode_values.append(info.get('current_price', 0))\n",
    "        \n",
    "        # 存储经验\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        # 经验回放学习\n",
    "        if len(replay_buffer) > batch_size * 2:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            states = torch.FloatTensor(states).to(device)\n",
    "            next_states = torch.FloatTensor(next_states).to(device)\n",
    "            actions = torch.LongTensor(actions).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).to(device)\n",
    "            dones = torch.FloatTensor(dones).to(device)\n",
    "            \n",
    "            # 计算目标Q值（Double DQN）\n",
    "            with torch.no_grad():\n",
    "                next_actions = policy_net(next_states).argmax(1)\n",
    "                next_q = target_net(next_states).gather(1, next_actions.unsqueeze(1))\n",
    "                target_q = rewards + gamma * next_q.squeeze() * (1 - dones)\n",
    "            \n",
    "            # 计算当前Q值\n",
    "            current_q = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "            \n",
    "            # Huber损失\n",
    "            loss = nn.SmoothL1Loss()(current_q, target_q)\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "    \n",
    "    # 更新目标网络\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    # 动态调整学习率\n",
    "    scheduler.step(total_reward)\n",
    "    \n",
    "    # 记录训练指标\n",
    "    portfolio = env.total_value\n",
    "    returns = np.diff(portfolio) / portfolio[:-1]\n",
    "    sharpe = np.mean(returns) / (np.std(returns) + 1e-9) * np.sqrt(240)\n",
    "    drawdown = (np.max(portfolio) - portfolio[-1]) / np.max(portfolio)\n",
    "    \n",
    "    training_log['episode'].append(episode+1)\n",
    "    training_log['total_return'].append((portfolio[-1]/env.initial_balance-1)*100)\n",
    "    training_log['max_drawdown'].append(drawdown*100)\n",
    "    training_log['sharpe_ratio'].append(sharpe)\n",
    "    training_log['epsilon'].append(epsilon)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if total_reward > best_score:\n",
    "        best_score = total_reward\n",
    "        torch.save(policy_net.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    # 衰减epsilon\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    \n",
    "    # 打印训练进度\n",
    "    if (episode+1) % 50 == 0:\n",
    "        print(f\"Episode {episode+1}/{episodes}\")\n",
    "        print(f\"Total Return: {training_log['total_return'][-1]:.2f}%\")\n",
    "        print(f\"Sharpe Ratio: {training_log['sharpe_ratio'][-1]:.2f}\")\n",
    "        print(f\"Max Drawdown: {training_log['max_drawdown'][-1]:.2f}%\")\n",
    "        print(f\"Epsilon: {epsilon:.3f}\\n\")\n",
    "\n",
    "# 绘制训练曲线\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(training_log['episode'], training_log['total_return'])\n",
    "plt.title('Total Return (%)')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(training_log['episode'], training_log['sharpe_ratio'])\n",
    "plt.title('Sharpe Ratio')\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(training_log['episode'], training_log['max_drawdown'])\n",
    "plt.title('Max Drawdown (%)')\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(training_log['episode'], training_log['epsilon'])\n",
    "plt.title('Epsilon Decay')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e399e-bbc7-498b-aae5-2dff5ea44d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 模型测试与交易分析\n",
    "def enhanced_test_model(env, model_path, scaler, features):\n",
    "    # 加载最佳模型\n",
    "    model = EnhancedDQN(state_size, action_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # 运行测试\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    trade_records = []\n",
    "    portfolio_values = []\n",
    "    \n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        portfolio_values.append(env.total_value[-1])\n",
    "        \n",
    "        if action != 0:  # 记录交易\n",
    "            trade_records.append({\n",
    "                'time': env.data.index[env.index-1],\n",
    "                'action': 'Buy' if action == 1 else 'Sell',\n",
    "                'price': info.get('current_price', 0),\n",
    "                'shares': env.trades[-1][3] if env.trades else 0,\n",
    "                'portfolio_value': portfolio_values[-1]\n",
    "            })\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # 可视化结果\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # 价格曲线\n",
    "    plt.subplot(3,1,1)\n",
    "    plt.plot(feature_data.index[:len(portfolio_values)], \n",
    "             scaler.inverse_transform(processed_data[:len(portfolio_values)])[:, 3], \n",
    "             label='Price')\n",
    "    plt.title('Price Movement')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 组合价值\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(feature_data.index[:len(portfolio_values)], portfolio_values, label='Portfolio')\n",
    "    plt.title(f'Portfolio Value (Final: {portfolio_values[-1]:.2f})')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 交易信号\n",
    "    plt.subplot(3,1,3)\n",
    "    buy_times = [t['time'] for t in trade_records if t['action'] == 'Buy']\n",
    "    sell_times = [t['time'] for t in trade_records if t['action'] == 'Sell']\n",
    "    plt.scatter(buy_times, [scaler.inverse_transform(processed_data[i])[3] \n",
    "               for i in range(len(buy_times))], color='g', label='Buy')\n",
    "    plt.scatter(sell_times, [scaler.inverse_transform(processed_data[i])[3] \n",
    "                for i in range(len(sell_times))], color='r', label='Sell')\n",
    "    plt.title('Trade Signals')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 生成交易报告\n",
    "    initial_value = env.initial_balance\n",
    "    final_value = portfolio_values[-1]\n",
    "    total_return = (final_value / initial_value - 1) * 100\n",
    "    num_trades = len(trade_records)\n",
    "    win_rate = len([t for t in trade_records if t['portfolio_value'] > initial_value]) / num_trades if num_trades >0 else 0\n",
    "    \n",
    "    print(f\"\\n{' Backtest Report ':=^40}\")\n",
    "    print(f\"初始资金: {initial_value:.2f}\")\n",
    "    print(f\"最终资金: {final_value:.2f}\")\n",
    "    print(f\"总收益率: {total_return:.2f}%\")\n",
    "    print(f\"交易次数: {num_trades}\")\n",
    "    print(f\"胜率: {win_rate*100:.2f}%\")\n",
    "    print(f\"最大回撤: {training_log['max_drawdown'][-1]:.2f}%\")\n",
    "    print(f\"夏普比率: {training_log['sharpe_ratio'][-1]:.2f}\")\n",
    "    \n",
    "    return trade_records\n",
    "\n",
    "# 运行测试\n",
    "test_env = EnhancedStockTradingEnv(processed_data)\n",
    "trade_log = enhanced_test_model(test_env, 'best_model.pth', scaler, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc82972-e45b-4ceb-8995-c05ed14b02e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
